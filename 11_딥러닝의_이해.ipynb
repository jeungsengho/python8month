{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_딥러닝의 이해.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNUrF65uG9bZpB+QumOrels",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeungsengho/python8month/blob/main/11_%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%98_%EC%9D%B4%ED%95%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 신경망"
      ],
      "metadata": {
        "id": "dyFflzG-Vtlq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. 인공지능, 머신러닝, 딥러닝의 포함 관계\n",
        "\n",
        "<img src='https://tensorflowkorea.files.wordpress.com/2018/12/028.jpg'>\n",
        "\n",
        "- AI > 머신러닝(SVM, 선형회귀, 로지스틱 회귀, 퍼셉트론..), 규칙기반 > 신경망(DNN, RNN, CNN, GAN..)\n",
        "- 신경망은 머신러닝 알고리즘 중 하나\n"
      ],
      "metadata": {
        "id": "TzrBBN4hVx1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. 신경망의 특징\n",
        "- 인간의 뇌기능을 흉내내려고 만들어짐\n",
        "- 입력층, 은닉층, 출력층으로 나누어져 있음\n",
        "- 가중치: 이 정보가 얼마나 더 중요한지(관련이 있는지)를 표현하기 위해\n",
        "- 심층 신경망(Neural Network): 층을 점점 늘려서 깊게 만든 신경망\n",
        "- 딥러닝(심층학습): 깊은 층을 가진 신경망의 가중치를 학습시키는 것"
      ],
      "metadata": {
        "id": "Zxs-VEGyV3QP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-3. 신경망으로 할 수 있는 것\n",
        "* 회귀\n",
        "* 분류\n",
        "* 이미지 생성"
      ],
      "metadata": {
        "id": "A_Trq0FAZd4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 신경망의 진화"
      ],
      "metadata": {
        "id": "FxJYwAqykNKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 1950년대 아이디어(퍼셉트론)가 나옴\n",
        "* 1980년대 오차역전파법으로 신경망 학습을 개발 -> 학습 데이터 부족으로 문제가 발생\n",
        "* 기울기 소실: 역전파 과정에서 입력층으로 갈 수록 기울기가 점차적으로 작아져서 최적의 모델을 찾을 수 없는 현상\n",
        "* 2000년대 인터넷이 보급된 후 많은 데이터를 얻을 수 있게 되면서 다시 주목받게 됨\n",
        "* 2012년\n",
        "  * CNN 중 Alex net라는 네트워크 구조\n",
        "  * Alex net이 이미지 분류 대회에서 1등을 차지 -> CNN이 유명해짐\n",
        "* 2013년\n",
        "  * 아타리 게임 중 '벽돌깨기' -> AI 적용 -> 학습을 많이 진행 -> 구석을 파서 공을 위로 올리는 방법을 깨닫게 됨\n",
        "  * 딥마인드 -> 구글에 인수 -> 알파고\n",
        "* 2014년\n",
        "  * RNN이라는 네트워크를 사용하여 중국어 -> 영어로 번역\n",
        "  * 성능의 한계가 생김\n",
        "  * attention 모델의 출현으로 성능이 급격히 좋아짐\n",
        "* 2015년\n",
        "  * GANs -> 오바마 합성사진\n",
        "  * ResNet(Residual Networks): CNN의 종류 중 하나, 사람과 AI가 이미지를 찾는 테스트를 해서 사람은 5%, ResNet은 3%의 오차율\n",
        "* 2016년\n",
        "  * 알파고를 통해 이세돌 9단을 이김\n",
        "  * 이세돌 9단은 컴퓨터를 이긴 마지막 인류\n",
        "* 2017년\n",
        "  * RNN의 단점을 극복한 attention만으로 만든 모델 -> Transformer\n",
        "  * Transformer: 번역 모델\n",
        "  * 자연어를 정복\n",
        "  * Transforemr는 딥러닝에서 가장 중요한 모델 중 하나\n",
        "* 2018년\n",
        "  * Transformer에서 인코더만 따온 모델 -> BERT\n",
        "  * 자연어를 정답없이 사용(인터넷 문장등의 데이터를 삽입)\n",
        "  * 일부 글자를 가리고 가린 부분을 맞출 수 있도록 학습\n",
        "  * 모델을 크게 만들고 엄청난 데이터를 사용해야 함\n",
        "* 2019/2020년\n",
        "  * GPU 수천대를 사용하여 큰 모델을 만듬\n",
        "  * 정답이 필요 없이 실제 데이터만으로 셀프 수퍼바이즈러닝\n"
      ],
      "metadata": {
        "id": "fIOTrBlikXgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 딥러닝의 키포인트"
      ],
      "metadata": {
        "id": "D6Q-k5_bslKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 데이터(양질, 많은 양)\n",
        "* 모델(CNN, RNN, Transformer, multilayer perceptron..)\n",
        "* 알고리즘(Gradient Descent를 기초로 많은 알고리즘이 만들어짐)\n",
        "* Loss Function"
      ],
      "metadata": {
        "id": "6WlJtPDgsnNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. NN(Neural Network)에서의 핵심"
      ],
      "metadata": {
        "id": "wMZt-qsNt_d5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Deep Neural Network: layer(은닉)수가 최소 2개 이상인 network\n",
        "* Loss Function, Cost Function: Neural Network가 얼마나 성능이 좋은지 또는 나쁜지에 대한 척도\n",
        "* loss 값을 줄이는 방법 -> 적절한 weight를 찾아야 함"
      ],
      "metadata": {
        "id": "w6hwjqsguF5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vWaK0EeZuu18"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}