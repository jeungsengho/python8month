{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12_선형회귀.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM8Dac15m2V6auPOHmLTYy+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeungsengho/python8month/blob/main/12_%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qTPdSX9hxUUD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터 셋\n",
        "x_train = torch.FloatTensor([[1], [2], [3], [4]])\n",
        "y_train = torch.FloatTensor([[50], [70], [90], [85]])"
      ],
      "metadata": {
        "id": "qsSBDPYBxnCD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y = W(Weight)x + b(bias)\n",
        "\n",
        "'''\n",
        "y = 20x + 15\n",
        "\n",
        "시간      1     2     3     4\n",
        "실제값    50    70    90    85\n",
        "예측값    35    55    75    95\n",
        "-------------------------------\n",
        "오차값    15    15    15    -10\n",
        "'''\n",
        "\n",
        "# MSE(평균 제곱 오차: Mean squared error)\n",
        "# 오차를 제곱하고 평균으로 나눈 것\n",
        "\n",
        "# 코드를 재실행해도 같은 랜덤 결과가 나옴\n",
        "torch.manual_seed(10)\n",
        "\n",
        "print(x_train)\n",
        "print(x_train.shape)\n",
        "\n",
        "print(y_train)\n",
        "print(y_train.shape)\n",
        "\n",
        "# 선형회귀의 핵심은 학습데이터와 가장 잘 맞는 직선을 찾는 작업\n",
        "# requires_grad=True: 학습을 통해 변경되는 변수\n",
        "# y = 0*x + 0\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "print(W)\n",
        "\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "print(b)\n",
        "\n",
        "H = x_train * W + b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqAJQN_lyXRU",
        "outputId": "24e02534-0e70-4a18-d9b9-4f046fc2a5ba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [4.]])\n",
            "torch.Size([4, 1])\n",
            "tensor([[50.],\n",
            "        [70.],\n",
            "        [90.],\n",
            "        [85.]])\n",
            "torch.Size([4, 1])\n",
            "tensor([0.], requires_grad=True)\n",
            "tensor([0.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 비용 함수 선언(cost function), 손실 함수(loss function), 오차 함수(error function)\n",
        "cost = torch.mean((H - y_train) ** 2)\n",
        "print(cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1xZ6d7h0LFV",
        "outputId": "ecdada88-69ad-4d1c-cbd0-57e1c43987a5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5681.2500, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 옵티마이저(optimizer)\n",
        "# 비용함수의 값을 최소로 하는 W(기울기)와 b(절편)을 찾는 방법(알고리즘)\n",
        "\n",
        "# 경사하강법(Gradient Descent)\n",
        "# 가장 기본적인 옵티마이저 알고리즘\n",
        "# cost가 최소화 되는 지점은 접선의 기울기가 0이 되는 지점이며 미분값이 0이 되는 지점\n",
        "# 비용함수를 미분하여 현재 W에서의 접선의 기울기를 구하고 접선의 기울기가 낮은 방향으로 W의 값을 업데이트 하는 작업을 반복\n",
        "\n",
        "# SGD(Stochastic Gradient Descent)\n",
        "# 배치 크기가 1인 경사하강법 알고리즘\n",
        "# 확률적 경사하강법은 데이터 셋에서 무작위로 균일하게 선택한 하나의 예를 의존하여 각 단계의 예측 경사를 계산\n",
        "\n",
        "# 학습률(learning rate)\n",
        "# 기울기의 값을 변경할 때 얼마나 크게 변경할지를 결정\n",
        "optimizer = optim.SGD([W, b], lr=0.01)"
      ],
      "metadata": {
        "id": "8wOJJOnj1ibt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient를 0으로 초기화\n",
        "optimizer.zero_grad()\n",
        "# 비용함수를 미분하여 gradient 계산\n",
        "cost.backward()\n",
        "# W와 b를 업데이트\n",
        "optimizer.step()"
      ],
      "metadata": {
        "id": "lIDmyras1zhF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 에폭(epoch): 전체 훈련 데이터가 학습에 한번 사용된 주기\n",
        "# 총 에폭을 2000번, 100번 마다 로그 출력\n",
        "\n"
      ],
      "metadata": {
        "id": "we9_KN1P4oF5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}